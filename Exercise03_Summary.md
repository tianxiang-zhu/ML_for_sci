# 🤖 Exercise 03 --- Basic Machine Learning Algorithms (lecture summary)

整理自 Pascal Friederich 《Lecture 5: Basics of Machine Learning Part
2》\
内容涵盖监督与非监督学习的核心算法。

------------------------------------------------------------------------

## 🧠 1. Maximum Likelihood Estimation (MLE) --- 最大似然估计

**核心思想：**\
从统计角度看学习算法，目标是找到能让观测数据最可能出现的模型参数 θ：

\[ `\hat{\theta}`{=tex} =
`\arg`{=tex}`\max`{=tex}\_`\theta `{=tex}p(x_1,...,x_m \|
`\theta`{=tex}) \]

常取对数化简：

\[ `\log `{=tex}L(`\theta`{=tex}) = `\sum`{=tex}\_i `\log `{=tex}p(x_i
\| `\theta`{=tex}) \]

若 p 为高斯分布，则 MLE 等价于最小化 MSE（线性回归的损失函数）。

------------------------------------------------------------------------

## ⚙️ 2. Gradient Descent --- 梯度下降优化

**目的：** 最小化损失函数 ( L(`\theta`{=tex}) )

**更新规则：** \[ `\theta`{=tex}\_{t+1} = `\theta`{=tex}*t -
`\eta `{=tex}`\nabla`{=tex}*`\theta `{=tex}L(`\theta`{=tex}\_t) \] 其中
η 为学习率。

**优缺点：** - ✅ 实现简单，可用于大规模数据； - ⚠️
可能陷入局部最优或振荡。

------------------------------------------------------------------------

## 📈 3. Logistic Regression --- 逻辑回归

**用途：** 二分类模型。输出为 0 -- 1 之间的概率。

**模型：** \[ p(y=1 \| x) = `\sigma`{=tex}(w\^T x + b) \] 其中 σ 为
Sigmoid 函数： \[ `\sigma`{=tex}(z)=`\frac{1}{1+e^{-z}}`{=tex} \]

**损失函数（交叉熵）** \[ L = - `\sum`{=tex}\_i \[y_i `\log `{=tex}p_i +
(1-y_i)`\log`{=tex}(1-p_i)\] \]

用 梯度下降 更新参数。\
在物理上与 Boltzmann 分布 ( p_i `\sim `{=tex}e\^{-E_i/kT}) 形式相似。

------------------------------------------------------------------------

## ⚖️ 4. Support Vector Machine (SVM) --- 支持向量机

**核心思想：** 在特征空间中找到一个能最大化两类间隔 margin 的超平面。

**线性可分形式：** \[ `\min`{=tex}\_{w,b} `\frac12`{=tex}\|\|w\|\|\^2
`\quad `{=tex}s.t. y_i(w\^T x_i + b) ≥ 1 \]

**非线性数据：** 使用 **Kernel Trick** 将输入映射到高维空间：

\[ K(x_i,x_j)=`\phi`{=tex}(x_i)\^T`\phi`{=tex}(x_j) \] 常见核函数： -
线性核 ( K=x_i\^T x_j ) - 多项式核 ( (γx_i\^T x_j + r)\^d ) - RBF/高斯核
( e^{-\|x_i-x_j\|^2/2σ\^2})

**优点：** - 稳定、泛化能力强； - 可用凸优化求解。

**缺点：** - 预测时间 ∝ 训练样本数（取决于支持向量数量）。

------------------------------------------------------------------------

## 📍 5. k-Nearest Neighbors (kNN) --- 近邻算法

**原理：** - 非参数方法； - 对未知样本，直接根据训练集中最近 k
个邻居的标签多数表决（分类）或平均（回归）； -
无需训练，预测阶段计算距离。

**距离度量：** \[ d(x,x')=`\sqrt{\sum_i(x_i-x'_i)^2}`{=tex} \]

**特点：** - 优点：简单、直观； - 缺点：高维空间中距离失效（维度灾难）。

------------------------------------------------------------------------

## 🌐 6. Curse of Dimensionality --- 维度灾难

在 N 维空间中，每维取 10 个值： - N=1 → 10 点\
- N=2 → 100 点\
- N=3 → 1000 点

随着维度上升，样本需求指数爆炸。\
高维数据稀疏，使得"邻近"概念失效，许多算法性能急剧下降。

------------------------------------------------------------------------

## 🧩 7. Unsupervised Learning --- 非监督学习简介

目标：从**无标签数据**中学习分布或结构。

常见形式： - 降维（Representation Learning）\
- 聚类（Clustering）

------------------------------------------------------------------------

## 📉 8. Principal Component Analysis (PCA)

**思想：** 线性变换数据，使新坐标轴方差最大且互不相关。

**步骤：** 1. 去均值； 2. 计算协方差矩阵； 3. 求特征值与特征向量； 4.
按方差大小选主成分。

**局限：** 仅能捕获线性关系，不能去除非线性依赖。

------------------------------------------------------------------------

## 🌀 9. k-Means Clustering --- K 均值聚类

**目标：** 将样本分为 k 个距离近的簇，使类内方差最小。

**算法步骤：** 1. 随机选 k 个初始中心； 2. 分配样本至最近中心； 3.
更新每个簇中心为均值； 4. 重复 2-3 直到收敛。

**数学表达：** \[ J=`\sum`{=tex}*i\|x_i - μ*{c_i}\|\^2 \]

**输出：** 每个样本的 one-hot 表示： ( h=(0,...,1,...,0))。

------------------------------------------------------------------------

## 🧭 10. Manifold Learning --- 流形学习

**思想：** 高维数据往往分布在嵌入空间中的低维流形上。\
流形学习旨在恢复这种低维结构。

示例： - 地球表面是 3D 空间中的 2D 流形； -
真实图像集合仅占像素空间的极小部分。

流形学习方法包括： - Isomap - t-SNE - UMAP

------------------------------------------------------------------------

## 💬 11. Key Takeaways

  算法                  类型       学习目标     优点         缺点
  --------------------- ---------- ------------ ------------ ----------
  Logistic Regression   监督学习   概率二分类   简单可解释   线性假设
  SVM                   监督学习   最大化间隔   泛化好       计算慢
  kNN                   监督学习   近邻投票     无训练       高维低效
  PCA                   非监督     降维         快速线性     仅线性
  k-Means               非监督     聚类         简单         依赖初值

------------------------------------------------------------------------

## 📚 12. 总结

1.  最大似然估计 (MLE) 提供了统一的概率学习框架；\
2.  梯度下降 是最常用的优化手段；\
3.  逻辑回归、SVM、kNN 体现了监督学习从线性到非线性的发展；\
4.  PCA、k-Means 展示了无监督学习的核心思想：结构发现与表示学习；\
5.  维度灾难与流形假设揭示了现代 ML 在高维数据上的挑战与突破方向。

------------------------------------------------------------------------

*Compiled for Exercise03.ipynb --- KIT Machine Learning for the Natural
Sciences*
