# ğŸŒ³ Decision Tree --- Exercise 02 (Lecture 2 Summary)

æ•´ç†è‡ª Pascal Friederichã€ŠLecture 02: Decision Trees as Interpretable ML
Modelsã€‹\
æ¯é“ç»ƒä¹ å¯¹åº”ä¸€ä¸ªæ ¸å¿ƒæœºå™¨å­¦ä¹ çŸ¥è¯†ç‚¹ã€‚

------------------------------------------------------------------------

## ğŸ§© 1. Entropyï¼ˆç†µï¼‰--- è¡¡é‡æ•°æ®é›†çš„ä¸çº¯åº¦

**å®šä¹‰ï¼š**\
ç†µ ( E(X) = - `\sum `{=tex}p_i `\log`{=tex}\_2 p_i )
è¡¡é‡ä¸€ä¸ªèŠ‚ç‚¹ä¸­æ ·æœ¬ç±»åˆ«çš„ä¸ç¡®å®šæ€§ã€‚\
è¶Šæ¥è¿‘ 0 â†’ èŠ‚ç‚¹è¶Šçº¯ï¼›è¶Šå¤§ â†’ èŠ‚ç‚¹è¶Šæ··ä¹±ã€‚

**Python å®ç°ï¼š**

``` python
def entropy(df, y_col):
    p = df[y_col].value_counts(normalize=True)
    return -np.sum(p * np.log2(p))
```

------------------------------------------------------------------------

## ğŸ§® 2. Information Gainï¼ˆä¿¡æ¯å¢ç›Šï¼‰--- è¡¡é‡ç‰¹å¾åˆ†è£‚è´¨é‡

**å®šä¹‰ï¼š** \$ IG = E(X) -
`\left`{=tex}(`\frac{|X_{left}|}{|X|}`{=tex}E(X\_{left}) +
`\frac{|X_{right}|}{|X|}`{=tex}E(X\_{right})`\right`{=tex}) \$

**è¦ç‚¹ï¼š** - ç†µå‡å°‘è¶Šå¤šï¼Œç‰¹å¾è¶Šé‡è¦ã€‚ - IG ç”¨äºé€‰æ‹©æœ€ä¼˜åˆ†è£‚ã€‚

**å®ç°æ€è·¯ï¼š** 1. ç®—åŸå§‹ç†µï¼› 2. ç®—å·¦å³å­é›†ç†µï¼› 3. åŠ æƒç›¸å‡ã€‚

------------------------------------------------------------------------

## âš™ï¸ 3. greedy_split() --- è´ªå¿ƒé€‰æ‹©æœ€ä¼˜åˆ†è£‚ç‚¹

**æ¦‚å¿µï¼š** åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šï¼Œé€‰æ‹©ä¿¡æ¯å¢ç›Šæœ€å¤§çš„ç‰¹å¾åŠé˜ˆå€¼ã€‚\
è¿™æ˜¯ä¸€ç§å±€éƒ¨æœ€ä¼˜ï¼ˆgreedyï¼‰ç­–ç•¥ã€‚

**å®ç°é€»è¾‘ï¼š** 1. éå†æ‰€æœ‰ç‰¹å¾ï¼›\
2. å¯¹æ¯ä¸ªç‰¹å¾çš„å¯èƒ½é˜ˆå€¼è®¡ç®—ä¿¡æ¯å¢ç›Šï¼›\
3. è®°å½•æœ€å¤§ IG çš„åˆ†è£‚æ–¹å¼ã€‚

------------------------------------------------------------------------

## ğŸŒ³ 4. DecisionTree ç±» --- æ„å»ºé€’å½’å†³ç­–æ ‘

**æ ¸å¿ƒæ–¹æ³•ï¼š** - `_fit()`ï¼šé€’å½’åˆ†è£‚æ•°æ®ç›´åˆ°çº¯èŠ‚ç‚¹æˆ–æ·±åº¦é™åˆ¶ï¼› -
`_add_node()`ï¼šåœ¨å›¾ç»“æ„ä¸­æ·»åŠ èŠ‚ç‚¹ä¿¡æ¯ï¼› - `_predict()`ï¼šé€’å½’éå†æ ‘åˆ†ç±»ã€‚

**èŠ‚ç‚¹å­˜å‚¨ä¿¡æ¯ï¼š** - ç‰¹å¾åä¸é˜ˆå€¼ï¼›\
- å½“å‰æ ·æœ¬æ•°ä¸ç†µï¼›\
- å·¦å³åˆ†æ”¯ã€‚

------------------------------------------------------------------------

## ğŸ¨ 5. plot() --- å†³ç­–æ ‘å¯è§†åŒ–

**ä½¿ç”¨ï¼š** åˆ©ç”¨ `networkx` + `pyvis` ç»˜åˆ¶äº¤äº’å¼å±‚çº§å›¾ï¼š

``` python
net = Network(height=900, width=900, directed=True, layout='hierarchical')
net.from_nx(self.G)
net.show('tree.html')
```

------------------------------------------------------------------------

## ğŸ“Š 6. Confusion Matrixï¼ˆæ··æ·†çŸ©é˜µï¼‰

**å®šä¹‰ï¼š** æ˜¾ç¤ºæ¨¡å‹åœ¨æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹åˆ†å¸ƒï¼š

  True â†“ / Predicted â†’   setosa   versicolor   virginica
  ---------------------- -------- ------------ -----------

**å®ç°ï¼š**

``` python
for true, pred in zip(pred[y_col], pred[f'{y_col}_pred']):
    confusion_matrix.loc[true, pred] += 1
# æˆ–
pd.crosstab(pred[y_col], pred[f'{y_col}_pred'])
```

------------------------------------------------------------------------

## âŒ 7. é”™è¯¯é¢„æµ‹æ•°ï¼ˆWrong Predictionsï¼‰

**è®¡ç®—å…¬å¼ï¼š** \$ wrong = total - correct = `\text{sum}`{=tex}(matrix) -
`\text{trace}`{=tex}(matrix) \$

**å®ç°ï¼š**

``` python
answer = int(confusion_matrix.values.sum() - np.trace(confusion_matrix.values))
```

------------------------------------------------------------------------

## ğŸ” 8. ç‰¹å®šåˆ†ç±»é”™è¯¯ç»Ÿè®¡

**é¢˜æ„ï¼š** \> æœ‰å¤šå°‘æ ·æœ¬è¢«é¢„æµ‹ä¸º versicolorï¼Œä½†çœŸå®æ˜¯ virginicaï¼Ÿ

**å®ç°ï¼š**

``` python
answer = int(confusion_matrix.loc['Iris-virginica', 'Iris-versicolor'])
```

------------------------------------------------------------------------

## ğŸ¯ 9. Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰

**å®šä¹‰ï¼š** \$ Accuracy =
`\frac{\text{æ­£ç¡®é¢„æµ‹æ•°}}{\text{æ€»æ ·æœ¬æ•°}}`{=tex} \$

**å®ç°ï¼š**

``` python
accuracy = np.trace(confusion_matrix.values) / confusion_matrix.values.sum()
```

------------------------------------------------------------------------

## ğŸ’¡ 10. å…¨ç« æ€»ç»“

1.  ç†µ (Entropy) è¡¡é‡ä¸ç¡®å®šæ€§ï¼›\
2.  ä¿¡æ¯å¢ç›Š (Information Gain) è¡¡é‡ç‰¹å¾åˆ’åˆ†ä»·å€¼ï¼›\
3.  å†³ç­–æ ‘é€šè¿‡è´ªå¿ƒåˆ†è£‚å®ç°é€’å½’åˆ†ç±»ï¼›\
4.  æ··æ·†çŸ©é˜µã€å‡†ç¡®ç‡æ˜¯æ¨¡å‹è¯„ä¼°çš„åŸºç¡€æŒ‡æ ‡ï¼›\
5.  å¯è§†åŒ–å¸®åŠ©ç†è§£æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

------------------------------------------------------------------------

*Compiled for Exercise02.ipynb --- KIT Machine Learning for the Natural
Sciences*
